{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2dd760cc",
   "metadata": {},
   "source": [
    "## Frozen Lake with value Iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34aa1845",
   "metadata": {},
   "source": [
    "- This is an implemention of **Value Iteration**, a core algorithm in dynamic programming for solving ***Markov Decision Processes (MDPs)***. \n",
    "- We will apply Value Iteration to the FrozenLake-v1 environment in Gymnasium to find the optimal policy that maximizes the\n",
    "expected reward.\n",
    "\n",
    "Value Iteration is a simple algorithm that combines both the Policy Evaluation and Policy Improvement\n",
    "steps. It iteratively updates the value function for each state until it converges to the optimal value\n",
    "function, then the policy can be derived from the value function.\n",
    "Overview:\n",
    "You will be working with the FrozenLake-v1 environment, a simple grid world where the agent has to\n",
    "move from the starting point to the goal while avoiding holes. The agent can take four actions (left,\n",
    "down, right, up). Your task is to:\n",
    "1. Apply Value Iteration to compute the optimal value function.\n",
    "2. Extract the optimal policy from the value function.\n",
    "3. Test the optimal policy by simulating the agent's behavior in the environment.\n",
    "Step-by-Step Instructions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7224266e",
   "metadata": {},
   "source": [
    "## Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c2c9c218",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install \"gymnasium[toy-text]\" --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7ec1fc61",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "813ec96b",
   "metadata": {},
   "source": [
    "## FrozenLake V1 environment (deterministic for simplicity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "abb729a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, {'prob': 1})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = gym.make(\"FrozenLake-v1\", render_mode=\"human\", is_slippery=False)\n",
    "env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42b89941",
   "metadata": {},
   "source": [
    "### Initialise Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ed91e5e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = 0.99 # Discount factor\n",
    "theta = 1e-6 # Convergence threshold\n",
    "value_table = np.zeros(env.observation_space.n) # Initialize value function for all states\n",
    "num_actions = env.action_space.n # Number of actions available in the environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2413eff2",
   "metadata": {},
   "source": [
    "### Implement Value Iteration Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c2f6c9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16,)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "def value_iteration():\n",
    "    while True:\n",
    "        delta = 0\n",
    "        \n",
    "        # Iterate over all states\n",
    "        for state in range(env.observation_space.n):\n",
    "            v = value_table[state]\n",
    "            max_value = float('-inf')\n",
    "            \n",
    "            # Iterate over all actions to find the maximum expected value for each state\n",
    "            for action in range(num_actions):\n",
    "                action_value = 0\n",
    "                # Sum over all possible next states \n",
    "                for prob, next_state, reward, done in env.env.P[state][action]:\n",
    "                    action_value += (reward + gamma * prob * value_table[next_state]))\n",
    "                    max_value = max(max_value, action_value)\n",
    "            # Update the value table for the current state\n",
    "            value_table[state] = max_value\n",
    "            delta = max(delta, abs(v - value_table[state]))\n",
    "            # Check for convergence\n",
    "        if delta < theta:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7caf44a8",
   "metadata": {},
   "source": [
    "## Extract the Optimal Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27f8858e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_policy():\n",
    "    # Initialize the policy array with zeros\n",
    "    policy = np.zeros(env.observation_space.n, dtype=int)\n",
    "    \n",
    "    # Iterate over all states\n",
    "    for state in range(env.observation_space.n):\n",
    "        action_values = np.zeros(num_actions)\n",
    "        \n",
    "        # Evaluate all actions for the current state\n",
    "        for action in range(num_actions):\n",
    "            action_value = 0\n",
    "            \n",
    "            # Sum over all possible next states\n",
    "            for prob, next_state, reward, done in env.env.P[state][action]:\n",
    "                action_value += prob * (reward + gamma * value_table[next_state])\n",
    "            \n",
    "            # Store the action value\n",
    "            action_values[action] = action_value\n",
    "        \n",
    "        # Choose the action with the highest value\n",
    "        policy[state] = np.argmax(action_values)\n",
    "    \n",
    "    return policy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e7573e1",
   "metadata": {},
   "source": [
    "## Visualizing the Optimal Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9547bd22",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimal_policy = extract_policy()\n",
    "print(\"Optimal Policy:\")\n",
    "print(optimal_policy.reshape((4, 4))) # Reshape to visualize as a 4x4 grid"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3801c1c0",
   "metadata": {},
   "source": [
    "## Test the Optimal Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5ab2f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "state = env.reset()[0]\n",
    "done = False\n",
    "total_reward = 0\n",
    "\n",
    "while not done:\n",
    "    # Follow the optimal policy\n",
    "    action = optimal_policy[state]\n",
    "    \n",
    "    # Take the action and observe the next state and reward\n",
    "    next_state, reward, done, _, _ = env.step(action)\n",
    "    \n",
    "    # Accumulate the total reward\n",
    "    total_reward += reward\n",
    "    \n",
    "    # Update the current state\n",
    "    state = next_state\n",
    "    \n",
    "    # Optional: Render the environment to visualize the agent's movements\n",
    "    env.render()\n",
    "\n",
    "# Print the total reward obtained by following the optimal policy\n",
    "print(f\"Total reward using optimal policy: {total_reward}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "reinforcement_learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
