{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dynamic Programming with GridWorld"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single-Row Gridworld\n",
    "\n",
    "This is a minimal reinforcement learning environment designed to introduce fundamental concepts of grid-based agents and navigation.\n",
    "\n",
    "### Grid Layout & Components\n",
    "\n",
    "```\n",
    "| S |   |   | G |\n",
    "```\n",
    "\n",
    "- **Total Cells**: 4\n",
    "- **Start State (S)**: First cell (index 0)\n",
    "- **Goal State (G)**: Last cell (index 3)\n",
    "\n",
    "### Agent Actions\n",
    "\n",
    "The agent can perform two basic actions:\n",
    "\n",
    "1. **Left**: Move one cell to the left\n",
    "   - If at the start state (S), no movement occurs\n",
    "2. **Right**: Move one cell to the right\n",
    "   - If at the goal state (G), no movement occurs\n",
    "\n",
    "### State Transition Rules\n",
    "\n",
    "#### Movement Constraints\n",
    "- Cannot move beyond grid boundaries\n",
    "- Attempting an invalid move keeps the agent in the current state\n",
    "\n",
    "### Action Dynamics\n",
    "\n",
    "#### Transition Probability\n",
    "- All actions are deterministic (100% chance of expected outcome)\n",
    "\n",
    "#### Reward Structure\n",
    "- **Default Reward**: 0 for non-terminal state transitions\n",
    "- **Goal Reward**: 1 when reaching the goal state\n",
    "\n",
    "### State Representation\n",
    "\n",
    "#### State Encoding\n",
    "- **States**: 0 (S), 1, 2, 3 (G)\n",
    "- **Terminal State**: State 3 (Goal state)\n",
    "\n",
    "### Mathematical Representation\n",
    "\n",
    "The environment can be formally described as:\n",
    "\n",
    "```\n",
    "action_i: {state_i: [(probability, next_state, reward, done)]}\n",
    "```\n",
    "\n",
    "Where:\n",
    "- `probability`: Likelihood of state transition (1.0 in this deterministic environment)\n",
    "- `next_state`: Resulting state after action\n",
    "- `reward`: Value received for the transition\n",
    "- `done`: Boolean indicating terminal state\n",
    "\n",
    "### Detailed Transition Dynamics\n",
    "- Left action from state 0: Remains at state 0\n",
    "- Right action from state 3: Remains at state 3\n",
    "- Other right/left actions move accordingly\n",
    "\n",
    "### Learning Objective\n",
    "\n",
    "The goal is to learn the optimal policy to reach the goal state with the maximum reward."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GridWorld:\n",
    "    def __init__(self):\n",
    "        self.n_states = 4  # We have 4 celles in the grid\n",
    "        self.n_actions = 2  # We have 2 actions (left and right)\n",
    "        # R = 0 for all transitions except terminal = 1\n",
    "        self.P = { \n",
    "            0: {\n",
    "                0: [\n",
    "                    (1.0, 0, 0, False)\n",
    "                ],  # from state 0, action 0 (left) leads to state 0 with reward 0. Agent bounced back and stayed in same position\n",
    "                1: [\n",
    "                    (1.0, 1, 0, False)\n",
    "                ],  # from state 0, action 1 (right) leads to state 1 on the right with reward 0\n",
    "            },\n",
    "            1: {\n",
    "                0: [\n",
    "                    (1.0, 0, 0, False)\n",
    "                ],  # from state 1, action 0 (left) leads to state 0 on the left with reward 0\n",
    "                1: [\n",
    "                    (1.0, 2, 0, False)\n",
    "                ],  # from state 1, action 1 (right) leads to state 2 on the right with reward 0\n",
    "            },\n",
    "            2: {\n",
    "                0: [\n",
    "                    (1.0, 1, 0, False)\n",
    "                ],  # from state 2, action 0 (left) leads to state 1 on the left with reward 0\n",
    "                1: [\n",
    "                    (1.0, 3, 1, True)\n",
    "                ],  # from state 2, action 1 (right) leads to state 3 on the right with reward 1 and done = True. Goal state reached\n",
    "            },\n",
    "            3: {  # This is a terminal state.  Agent stays in the same position with reard 0\n",
    "                0: [(1.0, 3, 0, True)],\n",
    "                1: [(1.0, 3, 0, True)],\n",
    "            },\n",
    "        }\n",
    "        \n",
    "    def get_trasition(self, state, action):\n",
    "        return self.P[state][action]\n",
    "    \n",
    "    \n",
    "env = GridWorld()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1.0, 1, 0, False)]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sample state transition\n",
    "\n",
    "env.get_trasition(2,0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.5, 0.5],\n",
       "       [0.5, 0.5],\n",
       "       [0.5, 0.5],\n",
       "       [0.5, 0.5]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a uniform random policy\n",
    "\n",
    "def create_policy():\n",
    "    policy = np.ones([env.n_states, env.n_actions]) / env.n_actions\n",
    "    return policy\n",
    "\n",
    "policy = create_policy()\n",
    "policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_evaluation(policy, env, gamma=0.3, theta=1e-6):\n",
    "    V = np.zeros(env.n_states)  # Initialize value functions to zero\n",
    "    while True:\n",
    "        delta = 0  # Track convergence\n",
    "        for s in range(env.n_states):\n",
    "            v = 0\n",
    "            for a, action_prob in enumerate(policy[s]):\n",
    "                for prob, next_state, reward, done in env.get_trasition(s, a):\n",
    "                    v += action_prob * (reward + gamma * prob * V[next_state])\n",
    "            delta = max(delta, abs(V[s] - v))\n",
    "            V[s] = v\n",
    "            \n",
    "        if delta < theta:  # Stop when value stabilizes\n",
    "            break\n",
    "    \n",
    "    return V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value function from policy evaluation:  [0.  0.  0.5 0. ]\n"
     ]
    }
   ],
   "source": [
    "V = policy_evaluation(policy, env, gamma=0.2, theta=1e6)\n",
    "\n",
    "print(\"Value function from policy evaluation: \", V)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Improvement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_improvement(V, env, gamma=0.9):\n",
    "    policy = np.zeros([env.n_states, env.n_actions])\n",
    "    \n",
    "    for s in range(env.n_states):\n",
    "        q_values = np.zeros(env.n_actions) # Action values\n",
    "        for a in range(env.n_actions):\n",
    "            for prob, next_state, reward, done in env.get_trasition(s, a):\n",
    "                q_values[a] += prob * (reward + gamma * V[next_state])\n",
    "        best_action = np.argmax(q_values)\n",
    "        policy[s, best_action] = 1.0  # Assign probability 1 to the best action\n",
    "    \n",
    "    return policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [1., 0.]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy_improvement(V, env, gamma=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "reinforcement_learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
